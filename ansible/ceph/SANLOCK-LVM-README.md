Управление кластерами Shared LVM на базе Sanlock
=====

Описание
--------

Плейбук предназначен для выполнения операций по настройке кластера:

- введение узлов в кластер
  - включение Watchdog
  - конфигурация LVM для работы с разделяемым блочным устройством
  - включение механизма замков
- удаление узла
- первичное создание VG

Вывод узла из LVM кластера приводит к исходному состоянию узла, за исключением работы watchdog демона, ибо его отключение приведёт к перегрузке сервера, однако после плановой перезагрузке служба Watchdog будет отключена.

> *Для корректной работы кластера следует, перед настройкой, сконфигурировать службу NTP. Это можно сделать с помощью соответствующего плейбука.*

Структура
---------

Плейбук инсталирован в каталог `/root/hvs-ansible` и имеет следующую структуру:

    group_vars/
        shared_lvm.yaml
        sslvm_0.yaml
    library/
        kmodule.py
    roles/
        sanlock_shared_lvm/
            defaults/
                main.yaml
            tasks/
                disable.yaml
                enable.yaml
                main.yaml
            templates/
                lvm.conf.j2
                lvmlocal.conf.j2
        watchdog/
            tasks/
                main.yaml
            README.md
    ansible.cfg
    hosts.yaml
    lvm_sanlock_shared.yaml
    lvm.yaml

Каталог `roles` содержит роли watchdog и sanlock_shared_lvm необходимые для работы плейбука.

Файлы `lvm_sanlock_shared.yaml` и `lvm.yaml`, являются текстом плейбука и его точкой входа соответственно.

Файл `ansible.cfg` содержит конфигурационную информацию для ansible.

Перечисленные далее составные части являются структурами управляющими поведением, и подлежат кастомизации.

Каталог `group_vars` должен содержать файлы с переменными плейбука, определяющими поведение. Изначально содержит пример настройки.

Файл `hosts.yaml` -- инвентори, перечисление узлов организованных в группы.

Настройки
---------

Плейбук работает с узлами внесёнными в группу с именем shared_lvm, каждый отдельный кластер должен быть выделен в самостоятельную группу. Вот пример содержимого `hosts.yaml`:

    all:
      children:
        shared_lvm:
          children:
            sslvm_0:
        sslvm_0:
          hosts:
            h110:
              ansible_host: host_ip_or_name
              user: root
            h109:
              user: root

Здесь в группу shared_lvm отнесена группа с именем sslvm_0. В свою очередь группа sslvm_0 содержит описание узлов составляющих отдельный кластер с общим ресурсом.

Далее, в каталоге `group_vars` файл `shared_lvm.yaml` содержит параметры, определяющие работу плейбука в части описания отдельных кластеров, вот его листинг:

    #
    # example inventory vars
    #

    sanlock_shared_lvm_groups:
    - sslvm_0

    ansible_python_interpreter: /usr/bin/python3

Здесь параметр `sanlock_shared_lvm_groups` содержит список групп определяющих отдельные кластера. остальное должно оставаться неизменным.

Следующим важным элементом являются файлы в каталоге `group_vars` с именами формата `имя_группы.yaml`, каждый должен содержать описание отдельного кластера, для примера приводим содержимое файла `sslvm_0.yaml`, содержащего параметры для группы sslvm_0:

    wd_enabled: true
    wd_module: softdog
    #wd_module_args: 
    lvm_shared_vgs:
      - name: store0
        pvs:
          - /dev/disk/by-id/scsi-36001405c3647f7263584fce9017278b5
    # - name: store2
    #   pvs:
    #     - /dev/vdd
    sslvm_enabled: true

Параметры `wd_enabled`, `wd_module` и `wd_module_args` определяют конфигурацию ядерной части службы Watchdog. В нашем примере указано что модуль **softdog** должен быть загружен (без дополнительных параметров).

Параметр `sslvm_enabled` в состоянии **true** указывает что нужно ввести узлы в состав кластера.

Параметр `lvm_shared_vgs` является перечислением разделяемых ресурсов которыми являются группы томов LVM, каждая группа представляется именем `name` и списком физических томов -- `pvs`.

Имена томов должны совпадать на всех узлах кластера, если это не так, то следует создать каталог `host_vars` и скопировав туда файл с параметрами группы переименовать его в соответствии с именем узла в инвентори (например sslvm_0.yaml -> h110.yaml). А затем произвести изменения отражающие отличия конкретно взятого узла.

Вышеописанным способом можно выводить отдельные узлы из состава кластера. Для этого в файле параметров узла следует не меняя других параметров изменить `wd_enabled` и `sslvm_enabled` на **false**

После внесения изменений следует сделать текущим каталог с плейбуком (`/root/hvs-ansible`) и запустить плейбук следующей командой:

    ansible-playbook lvm.yaml

> Хорошего вам дня
